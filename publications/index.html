<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Junlei Hu </title> <meta name="author" content="Junlei Hu"> <meta name="description" content="publications by categories in reversed chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://junleihu.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Junlei</span> Hu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJRR</abbr> </div> <div id="hu2025Autonomous" class="col-sm-8"> <div class="title">Autonomous Robotic Exploration on Unknown Soft Object</div> <div class="author"> <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a>, Gerardo Loza Galindo, <a href="https://profiles.uts.edu.au/Shoudong.Huang" rel="external nofollow noopener" target="_blank">Shoudong Huang</a>, and <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> The International Journal of Robotics Research (IJRR) (Major Revision Submitted) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/websites/exploration/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Robotic exploration of unknown soft objects presents significant challenges for autonomous systems due to unpredictable deformations and shape changes during manipulation. To address this, we propose a framework that integrates topology-aware 3D reconstruction with a topology-guided motion planner, enabling the discovery and reconstruction of previously hidden or concave regions. This topology-aware 3D reconstruction employs a novel representation of deformable objects by combining Cylinder Čech Complexes with point clouds, enabling rapid tracking of significant topology changes and detection of non-manifold boundaries.The topology analysis and canonical reconstruction guide motion planning by optimising grasp points and planning trajectories to reveal previously unseen surfaces through two actions: turning over and stretching. We validated our algorithm through simulations and experiments using the \textitda Vinci Research Kit, demonstrating successful exploration with two or three manipulators. We showed it can fully explore surfaces of two everyday objects, a beanie and a rubber glove, and two cadaveric organs, a liver and a colon, within seven manipulations. Our method achieved a 45.6% improvement in 3D reconstruction accuracy compared to state-of-the-art point-cloud-based methods while also demonstrating the capability to detect and fix non-manifold geometry.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>IEEE T-RO</div> </abbr> </div> <div id="hu2025Multiscale" class="col-sm-8"> <div class="title">Multiscale Deformable Objects Manipulation via Wavelet-Decomposed Boundary Element Method</div> <div class="author"> <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a>, and <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> Submitted to IEEE Transactions on Robotics (T-RO) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/websites/dwtbem/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Robotic deformable object manipulation faces critical challenges in surgical applications due to under-actuation, unpredictable tissue deformation, and limited intraoperative visibility. Traditional model-free methods often encounter unstable Jacobians caused by ill-conditioned observations, while physics-based models require precise tissue parameters and volumetric meshes, limiting their practicality in real surgeries. We propose a wavelet-Boundary Element Method (BEM) framework that encodes deformations from partial 3D shape feedback using multiscale wavelet descriptors. Coupling wavelets with BEM allows computation of the nonlinear motion-deformation mapping via boundary integrals, eliminating the need for volumetric meshes and enabling real-time control under occluded views. We validate our approach in simulation and on the da Vinci Research Kit (dVRK) using phantom organs and ex vivo animal tissue, achieving millimetre-level accuracy with contour, curve, and surface feedback. Comparative studies with Fourier-based and model-free methods show superior stability in dexterous tissue manipulation, addressing ill-conditioning through spatial-frequency localization. This framework enhances robotic manipulation in unstructured surgical environments, providing robust control and accurate tissue interaction despite partial observability. Our results demonstrate its potential to improve precision and safety in minimally invasive procedures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE T-MRB</abbr> </div> <div id="hu2025Instantaneous" class="col-sm-8"> <div class="title">Instantaneous and Markerless Registration for Mandible Reconstruction Robot with Fibular Holder</div> <div class="author"> <em>Junlei Hu</em>, Xiaoyan Jiang, Gang Zhu, Zhenggang Cao, Yao Gao, Zhaodong Zhang, Jing Han, and Jiannan Liu </div> <div class="periodical"> <em>IEEE Transactions on Medical Robotics and Bionics (T-MRB)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/11145193/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Robotic-assisted mandible reconstruction surgery is reported to achieve a more precise fibula osteotomy. However, intraoperative registration of the fibula with preoperative imaging is the key to achieving precision. Although methods such as probe scanning, optical navigation, and point cloud recognition are available, the reliability and efficiency of registration are low due to the sparse anatomical features and bone membrane covering on the fibular surface. Fibular shaping involves multiple bone segment osteotomies, and therefore, the design of fibular holder is necessary to achieve stable shaping. Based on these facts, an instantaneous and markerless registration based on the fibular holder is proposed, consisting of the initial alignment and refinement. The initial alignment using the point cloud or the holder to locate the fibula. The refinement, based on mechanical constraints, is developed and solved iteratively, utilizing the mechanical constraints between the fibula and the V-shaped mounts of the fibular holder. The accuracy of the approach is evaluated through simulations, model experiments and animal experiments. The results indicated that this algorithm provides higher accuracy and efficiency. This method can achieve non-invasive, efficient, and highly precise fibular registration in mandibular reconstruction surgery.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025Instantaneous</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Junlei and Jiang, Xiaoyan and Zhu, Gang and Cao, Zhenggang and Gao, Yao and Zhang, Zhaodong and Han, Jing and Liu, Jiannan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instantaneous and Markerless Registration for Mandible Reconstruction Robot with Fibular Holder}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Medical Robotics and Bionics (T-MRB)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="loza2025surg" class="col-sm-8"> <div class="title">Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision</div> <div class="author"> Gerardo Loza, <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a>, Sharib Ali, and <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2508.09681 (Submitted to IEEE Transactions on Medical Imaging)</em>, 2025 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2508.09681" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We proposed a novel test-time optimisation (TTO) approach framed by a NeRF-based architecture for long-term 3D point tracking. Most current methods in point tracking struggle to obtain consistent motion or are limited to 2D motion. TTO approaches frame the solution for long-term tracking as optimising a function that aggregates correspondences from other specialised state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose parametrising such a function with our new invertible Neural Radiance Field (InvNeRF) architecture to perform both 2D and 3D tracking in surgical scenarios. Our approach allows us to exploit the advantages of a rendering-based approach by supervising the reprojection of pixel correspondences. It adapts strategies from recent rendering-based methods to obtain a bidirectional deformable-canonical mapping, to efficiently handle a defined workspace, and to guide the rays’ density. It also presents our multi-scale HexPlanes for fast inference and a new algorithm for efficient pixel sampling and convergence criteria. We present results in the STIR and SCARE datasets, for evaluating point tracking and testing the integration of kinematic data in our pipeline, respectively. In 2D point tracking, our approach surpasses the precision and accuracy of the TTO state-of-the-art methods by nearly 50% on average precision, while competing with other approaches. In 3D point tracking, this is the first TTO approach, surpassing feed-forward methods while incorporating the benefits of a deformable NeRF-based reconstruction. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">loza2025surg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Loza, Gerardo and Hu, Junlei and Jones, Dominic and Ali, Sharib and Valdastri, Pietro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2508.09681 (Submitted to IEEE Transactions on Medical Imaging)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI 2024</abbr> </div> <div id="zeng2024realistic" class="col-sm-8"> <div class="title">Realistic surgical image dataset generation based on 3d gaussian splatting</div> <div class="author"> Tianle Zeng, Gerardo Loza Galindo, <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a>, and <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a> </div> <div class="periodical"> <em>In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-72089-5_48" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Computer vision technologies markedly enhance the automation capabilities of robotic-assisted minimally invasive surgery (RAMIS) through advanced tool tracking, detection, and localization. However, the limited availability of comprehensive surgical datasets for training represents a significant challenge in this field. This research introduces a novel method that employs 3D Gaussian Splatting to generate synthetic surgical datasets. We propose a method for extracting and combining 3D Gaussian representations of surgical instruments and background operating environments, transforming and combining them to generate high-fidelity synthetic surgical scenarios. We developed a data recording system capable of acquiring images alongside tool and camera poses in a surgical scene. Using this pose data, we synthetically replicate the scene, thereby enabling direct comparisons of the synthetic image quality (27.796\pm1.796 PSNR). As a further validation, we compared two YOLOv5 models trained on the synthetic and real data, respectively, and assessed their performance in an unseen real-world test dataset. Comparing the performances, we observe an improvement in neural network performance, with the synthetic-trained model outperforming the real-world trained model by 12%, testing both on real-world data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zeng2024realistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Realistic surgical image dataset generation based on 3d gaussian splatting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zeng, Tianle and Loza Galindo, Gerardo and Hu, Junlei and Valdastri, Pietro and Jones, Dominic}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{510--519}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>IEEE T-RO</div> </abbr> </div> <div id="hu2023occlusion" class="col-sm-8"> <div class="title">Occlusion-Robust Autonomous Robotic Manipulation of Human Soft Tissues With 3-D Surface Feedback</div> <div class="author"> <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a>, <a href="https://eps.leeds.ac.uk/computing/staff/743/professor-mehmet-dogar" rel="external nofollow noopener" target="_blank">Mehmet R Dogar</a>, and <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a> </div> <div class="periodical"> <em>IEEE Transactions on Robotics (T-RO)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10328689" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/websites/gpwrm/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Robotic manipulation of 3-D soft objects remains challenging in the industrial and medical fields. Various methods based on mechanical modeling, data-driven approaches or explicit feature tracking have been proposed. A unifying disadvantage of these methods is the high computational cost of simultaneous imaging processing, identification of mechanical properties, and motion planning, leading to a need for less computationally intensive methods. We propose a method for autonomous robotic manipulation with 3-D surface feedback to solve these issues. First, we produce a deformation model of the manipulated object, which estimates the robots’ movements by monitoring the displacement of surface points surrounding the manipulators. Then, we develop a 6-degree-of-freedom velocity controller to manipulate the grasped object to achieve a desired shape. We validate our approach through comparative simulations with existing methods and experiments using phantom and cadaveric soft tissues with the da Vinci research kit. The results demonstrate the robustness of the technique to occlusions and various materials. Compared to state-of-the-art linear and data-driven methods, our approach is more precise by 46.5% and 15.9% and saves 55.2% and 25.7% manipulation time, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2023occlusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Occlusion-Robust Autonomous Robotic Manipulation of Human Soft Tissues With 3-D Surface Feedback}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Junlei and Jones, Dominic and Dogar, Mehmet R and Valdastri, Pietro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics (T-RO)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{40}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{624--638}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA 2023</abbr> </div> <div id="hu2023coordinate" class="col-sm-8"> <div class="title">Coordinate Calibration of a Dual-Arm Robot System by Visual Tool Tracking</div> <div class="author"> <em>Junlei Hu</em>, <a href="https://eps.leeds.ac.uk/faculty-engineering-physical-sciences/staff/6971/dr-dominic-jones" rel="external nofollow noopener" target="_blank">Dominic Jones</a>, and <a href="https://eps.leeds.ac.uk/electronic-engineering/staff/863/professor-pietro-valdastri" rel="external nofollow noopener" target="_blank">Pietro Valdastri</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10161239" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The calibration of a vision-guided dual-arm robotic system, including the robot-robot and hand-eye calibration, requires the tracked positions of markers in different postures. However, in many cases, using markers to calibrate is impractical. Only some markerless features can be obtained rather than the rigid transform matrix; for example, the shaft of a markerless robotic tool can be tracked. Therefore, we proposed a Kronecker-Product-based method to calibrate the dual-arm system with a tracked robotic tool by decoupling the translation and rotation. The simulation and experiment results on a da Vinci Research Kit show that the proposed method is robust and accurate under different noise levels and various sample robot movements, compared with two state-of-the-art methods for dual-arm calibration with complete homogeneous transformations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hu2023coordinate</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Coordinate Calibration of a Dual-Arm Robot System by Visual Tool Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Junlei and Jones, Dominic and Valdastri, Pietro}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11468--11473}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://link.springer.com/journal/11548" rel="external nofollow noopener" target="_blank">IJCARS</a> </abbr> </div> <div id="li2020automatic" class="col-sm-8"> <div class="title">Automatic robot-world calibration in an optical-navigated surgical robot system and its application for oral implant placement</div> <div class="author"> Yang Li<sup>*</sup>, <em>Junlei Hu<sup>*</sup></em>, Baoxin Tao, Dedong Yu, Yihan Shen, Shengchi Fan, Yiqun Wu, and <a href="https://me.sjtu.edu.cn/teacher_directory1/chenxiaojun.html" rel="external nofollow noopener" target="_blank">Xiaojun Chen</a> </div> <div class="periodical"> <em>International Journal of Computer Assisted Radiology and Surgery (IJCARS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11548-020-02232-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Robot-world calibration, used to precisely determine the spatial relation between optical tracker and robot, is regarded as an essential step for optical-navigated surgical robot system to improve the surgical accuracy. However, these methods are complicated with numerous computation. Therefore, a more efficient method of a robot-world calibration is necessary. A fully automatic robot-world calibration was proposed and applied in a surgical robot system for oral implant placement. Making full usage of the movement characteristics of a tandem robot, the least square fitting algorithm was implemented to calculate the relationship between the tool center point of the robot and the robot reference frame, with the robot-world calibration matrix obtained as result.The experiment was designed to verify the accuracy of the robot-world calibration. The average distance deviation was 1.11 mm, and the average angle deviation was 0.99°. From the animal experiment on the pig maxilla, the entry, apical and angle deviation of the surgical robot system were 1.44±1.01 mm, 1.68±0.76 mm, 1.01±1.06°, respectively. The surgical robot system for oral implant placement with our robot-world calibration maintains a high precision. Besides, the operation range of the surgical tool is no longer limited by the visual range of the optical tracking device. Hence, it is unnecessary to adjust the optical tracking device for the planned implant trajectories to different positions and directions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">li2020automatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic robot-world calibration in an optical-navigated surgical robot system and its application for oral implant placement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yang and Hu, Junlei and Tao, Baoxin and Yu, Dedong and Shen, Yihan and Fan, Shengchi and Wu, Yiqun and Chen, Xiaojun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Assisted Radiology and Surgery (IJCARS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1685--1692}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://link.springer.com/journal/11548" rel="external nofollow noopener" target="_blank">IJCARS</a> </abbr> </div> <div id="hu2020approach" class="col-sm-8"> <div class="title">An approach to automated measuring morphological parameters of proximal femora on three-dimensional models</div> <div class="author"> <em>Junlei Hu</em>, Liyu Xu, Mengjie Jing, Henghui Zhang, Liao Wang, and <a href="https://me.sjtu.edu.cn/teacher_directory1/chenxiaojun.html" rel="external nofollow noopener" target="_blank">Xiaojun Chen</a> </div> <div class="periodical"> <em>International Journal of Computer Assisted Radiology and Surgery</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11548-019-02095-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Analyses of the morphology of proximal femora are essential for preoperative planning and designing customized femoral stems in total hip arthroplasty as well as intramedullary nailing fixation. Various studies reported measurements and analyses on the general geometry of proximal femora three-dimensionally. However, the modeling and measurements are time-consuming and unfriendly to surgeons. Thus, automated measurement and modeling of the femoral medullary canal are critical to promote the clinical application. An approach to automated measuring morphological parameters of proximal femur was proposed, and a software allowing importing femur models and manually locating the related anatomic landmarks was developed in the current study. 3D modeling of the femoral medullary canal was created by the semispherical iterative searching algorithm, and 16 key anatomic parameters of the proximal femur were automatically calculated by the least-squares fitting algorithm. By experimenting on 196 femur STL models, the average execution time of single measurement was 0.88 (SD = 0.13) s, and the intra-class correlation coefficient of 10 anatomic parameters was between 0.880 and 0.996, showing high intra-rater and inter-rater reliability. The parameters of proximal femur can be easily measured, and the 3D modeling of the femoral medullary canal can be rapidly achieved. The approach will be easily applicable to clinical practice and has the potential to be applied in the design of customized femoral stems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2020approach</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An approach to automated measuring morphological parameters of proximal femora on three-dimensional models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Junlei and Xu, Liyu and Jing, Mengjie and Zhang, Henghui and Wang, Liao and Chen, Xiaojun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Assisted Radiology and Surgery}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{109--118}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Junlei Hu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>